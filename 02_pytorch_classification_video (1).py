# -*- coding: utf-8 -*-
"""02_pytorch_classification_video.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HYe2dcmNuBbbxCmX3w7X6wBX-FPK-ETL
"""

#Importing all libraries

import sklearn
from sklearn.datasets import make_circles

"""

```
# This is formatted as code
```

# Binary Classification"""

n_samples = 1000

x,y = make_circles(n_samples,
                   noise=0.03,
                   random_state=42)

print("First 5 smaples of x",x[:5])
print("y",y[:5])

# Make a Dataframe of circle data
import pandas as pd
circles = pd.DataFrame({'x1':x[:,0],'x2':x[:,1],'label':y})
circles.head()

"""### Visualization"""

import matplotlib.pyplot as plt
plt.scatter(x = x[:,0],
            y = x[:,1],
            c = y,
            cmap = plt.cm.RdYlBu)

"""### Check input output shape"""

x.shape,y.shape

x

### View the samples and labels
X_sample = x[0]
y_sample = y[0]

print(X_sample.shape,y_sample.shape)

"""### Turn data into tensors"""

import torch
torch.__version__

X = torch.from_numpy(x).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

x[:5], y[:5]

X.dtype,y.dtype

# Split data into Train, test
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test  = train_test_split(X,
                                                     y,
                                                     test_size = 0.2,
                                                     random_state=42)

len(X_train), len(X_test)

"""## Building a model
#### Device
#### Construct model
#### Loss and optimizer
#### Create Train and Test loop
"""

import torch
from torch import nn

device = "cuda" if torch.cuda.is_available() else "cpu"
device

X_train.shape

# 1. Construct a model class
class Circle_model(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=5)
    self.layer_2 = nn.Linear(in_features=5, out_features=1)



  def forward(self,x):
    return self.layer_2(self.layer_1(x))

model_0 = Circle_model().to(device)
model_0

device

model_0.parameters

model_0 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5, out_features=1)
).to(device)

model_0.state_dict()
### Weights randomly instantiated

# Make predictions  ### Good Error
y_sample_predict_untrained = model_0(X_train)
y_sample_predict

# Make predictions
y_sample_predict_untrained = model_0(X_train[0].to(device))
y_sample_predict_untrained

with torch.inference_mode():
  y_sample_predict_untrained = torch.round(model_0(X_train[0].to(device)))
y_sample_predict_untrained

y_sample_predict_untrained==0

## LOSS AND optimizer
# BCELoss exists but requires inputs to have gone through the sigmoid activation function prior to inoout to BCELoss
## BCELossLogitsLoss is more numerically stable than BCELoss
loss_fn = nn.BCEWithLogitsLoss()

### OPtimizer

optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.1,
                            )

# Calculate accuracy

def accuracy(y_true,y_preds):
  y_true = y_true.squeeze()
  y_preds = y_preds.squeeze()
  correct = torch.eq(y_true,y_preds).sum().item()
  acc = (correct/len(y_preds))*100
  return acc



""" ### Train
 ##### Forward pass
 ##### Loss
 ##### optimization zero grad
 ##### backward pass
 #####  optimization step
"""

torch.manual_seed(42)
torch.cuda.manual_seed(42)
epochs = 1000


X_train, X_test = X_train.to(device), X_test.to(device)
Y_train, Y_test = Y_train.to(device), Y_test.to(device)

epoch_count = []
loss_train = []
acc_train = []

loss_test = []
acc_test = []

for epoch in range(epochs):

  model_0.train()

  # 1) Forward pass

  y_logits = model_0(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  #2) Calculate loss

  loss = loss_fn(y_logits, Y_train)

  acc = accuracy(y_true = Y_train,
                 y_preds=y_pred)

  #3) optimizer

  optimizer.zero_grad()

  #4) Loss

  loss.backward()

  #5) Optimizer step

  optimizer.step()

  epoch_count.append(epoch)
  loss_train.append(loss)
  acc_train.append(acc)


  #### Testing
  model_0.eval()

  with torch.inference_mode():
    #1) Forward Pass
    test_logits = model_0(X_test).squeeze()
    test_preds = torch.round(torch.sigmoid(test_logits))

    #2) Calculate test  loss/acc
    test_loss = loss_fn(test_preds, Y_test)

    test_acc = accuracy(y_true = Y_test,
                 y_preds=test_preds)


  loss_test.append(test_loss)
  acc_test.append(test_acc)
  # Printing out after every 10 epochs

  if epoch % 100 == 0:
        print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")



import requests
from pathlib import Path

# Download helper functions from Learn PyTorch repo (if not already downloaded)
if Path("helper_functions.py").is_file():
  print("helper_functions.py already exists, skipping download")
else:
  print("Downloading helper_functions.py")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
  with open("helper_functions.py", "wb") as f:
    f.write(request.content)

from helper_functions import plot_predictions, plot_decision_boundary

# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_0, X_train, Y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_0, X_test, Y_test)

"""#### Improving the model"""

# 1. Construct a model class
class Circle_model(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=128)
    self.layer_2 = nn.Linear(in_features=128, out_features=256)
    self.layer_3 = nn.Linear(in_features=256, out_features=1)
    self.relu = nn.ReLU()



  def forward(self,x):
    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))

model_0_improv = Circle_model().to(device)

## LOSS AND optimizer
# BCELoss exists but requires inputs to have gone through the sigmoid activation function prior to inoout to BCELoss
## BCELossLogitsLoss is more numerically stable than BCELoss
loss_fn = nn.BCEWithLogitsLoss()

### OPtimizer

optimizer = torch.optim.SGD(model_0_improv.parameters(), lr=0.1)

torch.manual_seed(42)
torch.cuda.manual_seed(42)
epochs = 1000


X_train, X_test = X_train.to(device), X_test.to(device)
Y_train, Y_test = Y_train.to(device), Y_test.to(device)

epoch_count = []
loss_train = []
acc_train = []

loss_test = []
acc_test = []

for epoch in range(epochs):

  model_0_improv.train()

  # 1) Forward pass

  y_logits = model_0_improv(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  #2) Calculate loss

  loss = loss_fn(y_logits, Y_train)

  acc = accuracy(y_true = Y_train,
                 y_preds=y_pred)

  #3) optimizer

  optimizer.zero_grad()

  #4) Loss

  loss.backward()

  #5) Optimizer step

  optimizer.step()

  epoch_count.append(epoch)
  loss_train.append(loss)
  acc_train.append(acc)


  #### Testing
  model_0_improv.eval()

  with torch.inference_mode():
    #1) Forward Pass
    test_logits = model_0_improv(X_test).squeeze()
    test_preds = torch.round(torch.sigmoid(test_logits))

    #2) Calculate test  loss/acc
    test_loss = loss_fn(test_logits, Y_test)

    test_acc = accuracy(y_true = Y_test,
                 y_preds=test_preds)


  loss_test.append(test_loss)
  acc_test.append(test_acc)
  # Printing out after every 10 epochs

  if epoch % 100 == 0:
        print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")

# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_0_improv, X_train, Y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_0_improv, X_test, Y_test)

test_loss

with torch.inference_mode():
    #1) Forward Pass
    test_logits = model_0_improv(X_test).squeeze()
    test_preds = torch.round(torch.sigmoid(test_logits))

    test_loss = loss_fn(test_preds, Y_test)

    test_acc = accuracy(y_true = Y_test,
                 y_preds=test_preds)

test_loss

len(Y_test),len(test_preds)



"""#Multiclass Classification"""

import torch
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
import torch.nn as nn

## Set the Hyperparameter
NUM_CLASSES = 4
NUM_FEATURES = 2
RANDOM_SEED = 42

device = 'gpu' if torch.cuda.is_available() else 'cpu'

device

# 1. Create Multi-class data
X_blob, Y_blob = make_blobs(n_samples=1000,
                            n_features=NUM_FEATURES,
                            centers =NUM_CLASSES,
                            cluster_std = 1.5,
                            random_state=RANDOM_SEED)
# 2. Turn data into Tensors
X_blob = torch.from_numpy(X_blob).type(torch.float)
Y_blob = torch.from_numpy(Y_blob).type(torch.float)

# 3. Split into Train and test
X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,
                                                                        Y_blob,
                                                                        test_size = 0.2,
                                                                        random_state =RANDOM_SEED)

X_blob.shape,X_blob[0],Y_blob.shape

X_blob.device

# 4 Visualize
plt.figure(figsize=(10,7))
plt.scatter(X_blob[:,0],X_blob[:,1],c = Y_blob,cmap = plt.cm.RdYlBu)

## Device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

class Blob_model(nn.Module):
  def __init__(self,input_features,op_features,hidden_units =8):
    super().__init__()
    self.linear_layer_stack = nn.Sequential(
        nn.Linear(in_features = input_features, out_features = hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units,out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units,out_features=op_features),
    )

  def forward(self,x):
    return self.linear_layer_stack(x)

### create model
model_4 = Blob_model(
    input_features = 2,
    op_features = 4,
    hidden_units = 128).to(device)

list(model_4.parameters())

#### Loss and optimizer

loss_fn = torch.nn.CrossEntropyLoss() # weight parameter in Crossentropy loss can be used during class inbalance

optimizer = torch.optim.SGD(params= model_4.parameters(),lr=0.1)

#
model_4.eval()
with torch.inference_mode():
  y_logit_test = model_4(X_blob_test.to(device))
  y_probs_test = torch.softmax(y_logit_test,dim=1)
  y_preds_test = torch.argmax(y_probs_test,dim=1)

y_preds_test[0:10]

x,y = make_circles(n_samples,
                   noise=0.03,
                   random_state=42)

X = torch.from_numpy(x).type(torch.float)
y = torch.from_numpy(y).type(torch.LongTensor)

X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X,
                                                                        y,
                                                                        test_size = 0.2,
                                                                        random_state =RANDOM_SEED)

torch.manual_seed(42)
torch.cuda.manual_seed(42)
epochs = 1000

X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)
X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)

for epoch in range(epochs):
  model_4.train()

  # 1.) Forward
  y_logits = model_4(X_blob_train)
  y_preds = torch.softmax(y_logits,dim=1).argmax(dim=1)

  # 2.) Loss
  loss = loss_fn(y_logits,y_blob_train)
  acc = accuracy(y_true =y_blob_train,
                 y_preds = y_preds)
  # 3.) Optimization zero
  optimizer.zero_grad()

  # 4.) Loss backprop
  loss.backward()

  # 5.) Optimization step

  optimizer.step()

  ### Testing

  model_4.eval()

  with torch.inference_mode():

    y_logits_test = model_4(X_blob_test)
    y_preds_test = torch.argmax(torch.softmax(y_logits_test,dim=1),dim=1)

    test_loss = loss_fn(y_logits_test,y_blob_test)
    test_acc = accuracy(y_true =y_blob_test,
                 y_preds = y_preds_test)

    if epoch % 100 == 0:
        print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")

y_logits.shape

test_acc = accuracy(y_true =y_blob_test,
                 y_preds = y_preds_test)

test_acc

# Plot decision boundaries for training and test sets
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_4, X_blob_train, y_blob_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_4, X_blob_test, y_blob_test)

